---
title: "MovieLens - Netflix Challenge"
author: "Aik Hean Lim"
date: "12/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Going through the Netflix's massive movie library aimlessly does not seem efficient. Therefore, it is a good idea for Netflix to have a good movie recommendation system. In their goal to improve their movie recommendation by 10%, Netflix made a call-out to the data science community to come up with a model to accurately predict viewer ratings.

Netflix's users are given the choice to rate their viewing experience from 0 to 5, 0 being the worse and 5 the best. The movie reommendation system was meant to recommend movies to users according to their preference.

The Netflix challenge selects their winner by comparing the residual mean square error (RMSE) of the prediction model for movie ratings. RMSE is common method to measure typical error loss, in order to obtained the desirable outcome. Hence, the most accurate prediction model should have the lowest RMSE when computing against the actual rating scores or validation set.

The main objective of this project is to generate a predicting model similar to the models from the Netflix challenge.

Accurately predicting movie ratings can be a complex task as there are many biases to be consider. 

It is safe to assume all ratings are not same for all movies and also from different users. Movies can be divide into "good" and "bad" movies, depending on one's preference.


## Data Wrangling

### Import and Tidy Data

The original Netflix movie database used in the Netflix's challenge was not made available for public access. However, it is possible to obtain a similar dataset from the courtesy of GroupLens research lab. This database contains up to 27,000 movies, with over 20 million ratings by more than 138,000 users.

To get started, the dataset mentioned in the paragraph above can be downloaded from the follwoing url:

[link](http://files.grouplens.org/datasets/movielens/ml-10m.zip)

The 10M MovieLens version is ideal for this analysis and we can download it with the following code:

```{r import-clean, include =  FALSE, error = FALSE, message=FALSE, warning=FALSE}
if(!require(tidyverse))
  install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) 
  install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) 
  install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.us.r-project.org")
if(!require(rafalib)) 
  install.packages("rafalib", repos = "http://cran.us.r-project.org")

# Download the 10M MovieLens dataset:
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

# Cleaning the dataset:
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId], title = as.character(title), genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
```

### Generate Validation and Train Set

The first step after data wrangling is to develop the prediction algorithm using the test set from the larger train set. Codes to generate train and test set can be found below:

```{r validation & train set, echo=FALSE}
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
mv_train <- movielens[-test_index,]
temp <- movielens[test_index,]

mv_test <- temp %>% 
  semi_join(mv_train, by = "movieId") %>%
  semi_join(mv_train, by = "userId")
removed <- anti_join(temp, mv_test)
mv_train <- rbind(mv_train, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

```{r, echo = TRUE}
dim(mv_train)
glimpse(mv_train)
head(mv_train)
```

```{r, echo = TRUE}
load('rdas/mv_train.rda')
load('rdas/mv_test.rda')
```

## The Dataset

The final dataset contain 10 million ratings (rows) with 6 variables (column).

The original 10M MovieLens dataset were partitioned into train and validation sets containing approximately 9 million and 1 million ratings respectively.

```{r mv_train, echo = FALSE, warning = FALSE, message = FALSE}
mv_train %>% as_tibble()
```

```{r mv_test, echo = FALSE, warning = FALSE, message = FALSE}
mv_test %>% as_tibble()
```


## Data Exploration

Every observation in the datasets indicate a rating entry by a user who rated a particular movie. However in reality, not every user rates every movie they watched. The number of distinct users and movies in the train set can be summarized with the following code:

```{r distinct-users-movies, echo=TRUE, message=FALSE, warning=FALSE}
mv_train %>% 
  summarize(users_count = n_distint(userId),
            movies_count = n_distinct(movieId))
```
Asssuming each user watches and rates every movie known in the dataset, then there would be over 700 millions ratings. The train set on the other hand, have about 10 millions observations. This disparity gives us the idea that thematrix might actually lack density.

To better visualize the matrix, a 100 $\times$ 100 matrix with random sampling will be generated with the code below:

```{r rating-matrix, echo=TRUE}
users_samp <- sample(unique(edx$userId), 100)
rafalib::mypar()
mv_train %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```

Note that tiny blocks in color within the image matrix above represents a movie rating from a user, whereas white space which fills the rest of the matrix represents otherwise. From this, we concluded that the movie-user rating matrix is indeed sparse.


## Distribution of variables

To view how the users and movies are distributed across the dataset, frequency plot can be used to visualize the distribution:

```{r rated-movie, echo=TRUE, message=FALSE, warning=FALSE}
mv_train %>% 
  dplyr::count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Movies") 
```

Movies with higher count are most likely blockbuster movies meanwhile those with lower count are most likely indie movies which are not known to many. 

```{r active-user, echo=TRUE, message=FALSE, warning=FALSE}
mv_train %>%
  dplyr::count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() +
  ggtitle("Users")
```

A glimpse of the user's behaviour shows some users habitually rate every movie they watched, while some might not rate every movie they watched. 


## Data Analysis

It is important to know that when predicting the rating for a movie *i* by user *u*, there is a noticeable pattern such as movie *i* tends to receive certain rating and user *u* tends to give certain rating, however, movies and users which are not movie *i* and user *u* tends to get different results. Therefore, the variety of information will allows to us to narrow down the choice of movies similar to movie *i* or users which have the similar rating behaviour to user *u*. In a nut shell, every information in the matrix may serve as a useful predictors for each predictions.

### Understanding the Loss Function

In short, the RMSE or residual mean squared error is the measure of accuracy for prediction algorithm. By taking $y_{u,i}$ as the actual rating of movie *i* given by user *u* and $\hat{y}_{u,i}$ as the prediction rating, with the same movie and user.

The formula of RMSE:

$$RMSE = \sqrt{\frac{1}{N}\sum_{u,i}^{} (\hat{y}_{u,i} - y_{u,i})^2}$$

Notes: *N* represent the sum of the movie/user combinations.

The function to compute the RMSE of actual sets and their corresponding predictors is written like this:

``` {r rmse function, echo = FALSE, warning = FALSE, message = FALSE}
RMSE <- function(true_ratings, pred_ratings){
  sqrt(mean((true_ratings - pred_ratings)^2))
}
```

### First Model: Naive model

The first approach to run the RMSE is to create a basic naive model which assume all ratings for every movies are the same, without any biases, like this:

$$Y_{u,i} = \mu + \epsilon_{u,i}$$

$\mu$ is defined as the mean or "true" rating for all movies.
$\epsilon_{u,i}$ is defined as the independent error observed at $\mu$.

This naive model will serve as a reference for the following models which will included the relevant biases to improve the prediction accuracy.

To standardized the ratings, the mean rating is calculated from the rating variable:

```{r mu_hat, echo = FALSE, warning = FALSE, message = FALSE}
rating_mu_hat <- mean(mv_train$rating)
rating_mu_hat
```

Calculation of the naive RMSE can be done with following code:

``` {r naive_rmse, echo = FALSE, warning = FALSE, message = FALSE}
naive_rmse <- RMSE(validation$rating, mu_hat)
naive_rmse
```

Note that the value of the naive RMSE is about 1, indicating the error of prediction is more than one star of the actual rating, and thus not a reliable prediction.

The next step is to identify the biases which are relevant to the prediction model such as which movie and which user rates gave the ratings.


### Second Model: Movie Effect Model

In general, not all movie are the same. Movies ratings depends on various factors, such as storyline, genre, quality and so on.

By adding the movie effect $b_{i}$ into the naive model, the new model will be:

$$Y_{u,i} = \mu + b_{i} + \epsilon_{u,i}$$

Besides that, a popular movie does not meant that it will have higher rating than a less popular one. Popularity and quality are examples of a movie factor which may influence the ratings given by users who watched it.

Thus, in this second model bias or effect that will be consider the movie-specific effect in order to reduce the RMSE.


```{r movie_avgs, echo = FALSE, warning = FALSE, message = FALSE}
mu <- mean(mv_train$rating)
movie_avgs <- mv_train %>% 
                group_by(movieId) %>% 
                summarize(b_i = mean(rating - mu))
```

```{r predicted-ratings, echo = FALSE,}
predicted_ratings <- mu + mv_test %>% 
                        left_join(movie_avgs, by='movieId') %>% pull(b_i)


# calculate rmse after modelling movie effect
model_2 <- RMSE(pred_ratings, mv_test$rating)
```


### Third Model: User Effect Model

Apart from movie effects *i*, user effect *u* is just as important and should be included with the movies effect model.

Take a closer look at the rating behaviour of the users, do all users gave a "good" blockbuster movie good ratings, over 4 to 5 stars?

``` {r user-bias, echo = FALSE, warning = FALSE, message = FALSE}
mv_train %>% group_by(userId) %>% 
            summarize(b_u = mean(rating)) %>% 
            filter(n() >= 100) %>% 
            ggplot(aes(b_u)) + 
            geom_histogram(bins = 30, color = "black")
```

Although a movie *i* was given high rating by most users, there are some users who gave averagely lower ratings compare to their high-rating-giver counterpart. This implies that user-specific effect is also a determining factor for the given movie rating. Adding user-specific effect b_{u} to the equation:

$$Y_{u,i} = \mu + b_{i} + b_{u} + \epsilon_{u,i}$$

Similar to movie effect, the user effect, *u*, can be computed as follows:

```{r user_avgs, echo = FALSE, warning=FALSE, message=FALSE}
user_avgs <- mv_train %>% 
                left_join(movie_avgs, by='movieId') %>%
                group_by(userId) %>%
                summarize(b_u = mean(rating - mu - b_i))}
```

Hence, the RMSE is further reduced to a lower value with the addition of the user-specific effect, b_{u}. 


## Regularization

From the previous model RMSE is reduced. However, the improvement of the RMSE was insignificant despite having a large movie to movie variability.

To identify any mistake which were overlookded from creating the user-effect only model. Starting from the movies which are given rather high ratings:


The list of movies are given high ratings, but most of movie titles are almost unheard of.

Creating the movie library 

These are amongst the 10 best and worse movies according to our users:

As mentioned before, these are the movies which are rated or watched by a handful of users, in this case, mostly by just one user, whom gave the movies their best or worse possible ratings.

These are the noisy factors which can disrupt the estimates by producing large errors, therefore increasing the RMSE. This uncertainty will cause the computation to produce larger estimates of b_{i}, either being more positive or more negative.

In this situation, regularization should be used to the fit the model

$$Y_{u,i} = \mu + b_i + \varepsilon_{u,i}$$

Regularization is a method to limit the variability by penalizing large estimates which are produced from small sample sizes. 

In previous sections, we computed standard error and constructed confidence intervals to account for different levels of uncertainty. However, when making predictions, we need one number, one prediction, not an interval. For this, we introduce the concept of regularization.


### Fourth Model: Regularized Movie + User Effect Model

Regularizing movie and user effect in one model:

$$\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + \lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)$$

Combine both effect in a single model
```{r lambda, echo = FALSE, message=FALSE, warning=FALSE}
# choosing the penalty term lambda
lambdas <- seq(0, 10, 0.5)
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(mv_train$rating)
  
  b_i <- mv_train %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- mv_train %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predict_ratings <- 
      mv_test %>% 
      left_join(b_i, by = "movieId") %>%
      left_join(b_u, by = "userId") %>%
      mutate(pred = mu + b_i + b_u) %>%
      pull(pred)
  
  return(RMSE(predicted_ratings, validation$rating))
})
qplot(lambdas, rmses) 
```

Selecting the lambda value which produces the minimum value of RMSE:

``` {r optimal-lambda, echo = TRUE, message = FALSE, warning = FALSE}
lambda <- lambdas[which.min(rmses)]
lambda

# RMSE of regularized movie + user effect model
rmse_4 <- min(rmse_f)
```

# Results

By running the models step by step, the RMSE gradually decreases from the first naive model after computing the biases into the model.

By running the models step by step, the RMSE gradually decreases from the model to model. From the basic na?ve model, the computed RMSE **(RMSE = 1.0612018)** does not seems satisfying as this model produced an error more than a star to the prediction model. Then, the second model (movie effect model) shown a significant drop in RMSE **(RMSE = 0.9439087)** from the first model (naive model) by computing the movie effect bias into the model. Again, significant reduction of RMSE **(RMSE = 0.8653488)** seen from the third model, with the computation of movie and user effect combination. To prevent model from producing larger estimates with small sample size, the method of regularization is then used to further reduce the RMSE. The fourth model (regularized movie + user effect) produced **(RMSE = 0.8648177)**.

The results can be summarized into a table below:

| Model | RMSE |
| ---------------------------------|-------------|
| Naive Model | `naive_rmse` |
| Movie effect Model | `rmse_2` |
| Movie + User effect Model | `rmse_3` |
| Regularized MOvie + User effect Model | `rmse_4` |

## Conclusion

Is is crucial to achieve the lowest possible RMSE when computing model to predict movie ratings, as it will reduce the prediction error of the model. Depending on the effect, the prediction model will obtained the desire outcome.

## References
* https://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest

* http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary

* https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf

* https://courses.edx.org
